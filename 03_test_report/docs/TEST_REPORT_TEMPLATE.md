# 📊 테스트 결과 보고서 (Test Result Report)

본 프로젝트의 RAG(Retrieval-Augmented Generation) 시스템 및 주요 기능에 대한 테스트 결과 보고서입니다.

---

## 1. 테스트 개요

- **시스템 명**: 미국 주식 분석 RAG 챗봇 서비스
- **테스트 일시**: 2026-02-02
- **테스트 환경**:
  - **LLM**: gpt-4.1-mini (AnalystChatbot 기본 모델)
  - **Embedding**: text-embedding-3-small
  - **데이터 소스**: S&P 500 기업 10-K 보고서 및 Finnhub 실시간 데이터

---

## 2. RAG 데이터셋 생성 결과

`03_test_report/generate_dataset.py`를 통해 생성된 평가용 합성 데이터셋 현황입니다.

| 항목 | 수치 | 비고 |
| :--- | :--- | :--- |
| 총 문서 수 | 500+ (S&P 500 전수) | 10-K CSV Files |
| 생성된 질문 수 | 50개 (기본 설정) | Ragas 기반 |

---

## 3. RAG 성능 평가 결과 (Evaluation)

`03_test_report/generate_report_summary.py` 실행 결과 요약입니다. (상세 내역: `03_test_report/data/evaluation_results_ragas.csv`)

| 지표 (Metrics) | 결과 | 목표치 |
| :--- | :--- | :--- |
| Faithfulness | 0.0750 | > 0.8 |
| Answer Relevancy | 0.7303 | > 0.8 |
| Context Recall | 0.0950 | > 0.7 |
| Context Precision | 0.0857 | > 0.7 |

#### 📝 지표 설명

- **Faithfulness (신뢰성)**: 생성된 답변이 검색된 컨텍스트(문서)에 얼마나 충실한지 측정합니다. (0.075는 답변이 컨텍스트에 없는 내용을 포함하거나 환각 현상이 있음을 시사)
- **Answer Relevancy (답변 관련성)**: 생성된 답변이 질문의 의도에 얼마나 부합하는지 측정합니다. (0.73은 질문에 대해 적절한 답변을 제공하고 있음을 의미)
- **Context Recall (문서 재현율)**: 정답(Ground Truth)을 생성하는 데 필요한 정보가 검색된 컨텍스트에 얼마나 포함되었는지 측정합니다. (0.095는 핵심 문서가 제대로 검색되지 않았음을 의미)
- **Context Precision (문서 정밀도)**: 검색된 컨텍스트 중 실제 문제 해결에 도움이 되는 문서의 비율입니다. (0.085는 검색 결과에 불필요한 노이즈가 많음을 의미)

### 상세 결과 (샘플 5건)

| user_input | faithfulness | answer_relevancy |
| :--- | ---: | ---: |
| 메디케이드 상환 수준과 지불 모델의 변화가 의료 제공자와 의약품 유통 부문에 어떤 영향을 미쳤습니까? | 0.0714286 | 0.854374 |
| QuickBooks가 고객을 위해 하는 일은 무엇인가요? | 0 | 0.836097 |
| 재무 토론 및 분석에 따르면 퍼스트 솔라(FSLR)는 어떤 산업과 부문에서 활동하고 있습니까? | 0.5 | 0.784451 |
| 아메리칸 익스프레스는 신용 서비스 운영 내에서 규제 준수를 어떻게 관리합니까? | 0 | 0.820026 |
| CSX와 같은 철도 회사들은 사이버 공격과 기술 업그레이드를 어떻게 대응하나요? | 0.266667 | 0.888008 |

---

## 4. 개별 기능 테스트 (Unit/Integration)

### 🔐 사용자 인증 및 세션 관리

- [x] 회원가입 및 로그인 기능 정상 작동
- [x] 세션 유지 및 자동 로그인 검증

### ⭐ 즐겨찾기(Watchlist) 기능

- [x] 기업 추가/삭제 기능 (Supabase 연동)
- [x] 챗봇 도구를 통한 실시간 반영

---

## 5. 결론 및 향후 개선 사항

- **결론**: Answer Relevancy는 준수하나(0.73), **Faithfulness(0.075) 및 Context Recall/Precision이 매우 낮음**.
- **원인 분석**:
  - LLM이 답변 생성 시 검색된 컨텍스트(Context)를 충분히 활용하지 못하거나, 검색된 문서의 품질이 낮을 가능성 존재.
  - 또는 평가 데이터의 Ground Truth와 검색된 문서 간의 정합성 문제 가능성.
- **개선 계획**:
  1. **검색 품질 개선**: Hybrid Search 가중치 조절 및 Re-ranking 도입 고려.
  2. **프롬프트 엔지니어링**: 답변 생성 시 컨텍스트 의존도를 높이도록 시스템 프롬프트 수정.
  3. **평가 데이터셋 검증**: Ground Truth가 문서 내에 명확히 존재하는지 재확인.
