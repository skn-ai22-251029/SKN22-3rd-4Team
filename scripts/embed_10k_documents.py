"""
10-K ë¬¸ì„œ ì„ë² ë”© ë° Supabase ì—…ë¡œë“œ (Vector Store)

ìˆ˜ì§‘ëœ 10-K í…ìŠ¤íŠ¸ íŒŒì¼(data/10k_documents/)ì„ ì½ì–´ì™€ì„œ
ì²­í‚¹(Chunking) í›„ OpenAI ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""
import os
import time
from pathlib import Path
from typing import List, Dict
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# ì„¤ì •
DATA_DIR = Path("data/10k_documents")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_API_KEY]):
    raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜(.env)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
)

def get_embedding(text: str) -> List[float]:
    """OpenAI ì„ë² ë”© ìƒì„±"""
    text = text.replace("\n", " ")
    return openai_client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding

def process_company_documents(ticker: str, directory: Path):
    """íŠ¹ì • ê¸°ì—…ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ì—…ë¡œë“œ"""
    print(f"\nğŸ“„ {ticker} ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    
    # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡
    files = {
        "business": directory / "business.txt",
        "risk_factors": directory / "risk_factors.txt",
        "mda": directory / "mda.txt"
    }
    
    documents = []
    
    for section, file_path in files.items():
        if not file_path.exists():
            continue
            
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
            
        if not text:
            continue
            
        # 1. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = text_splitter.split_text(text)
        print(f"   - {section}: {len(chunks)} chunks")
        
        # 2. ì„ë² ë”© ë° ë°ì´í„° ì¤€ë¹„
        for i, chunk in enumerate(chunks):
            documents.append({
                "ticker": ticker,
                "content": chunk,
                "metadata": {
                    "section": section,
                    "chunk_index": i,
                    "source": "10-K"
                }
            })
    
    if not documents:
        print("   âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. ë°°ì¹˜ ì—…ë¡œë“œ (OpenAI Rate Limit ë° ë„¤íŠ¸ì›Œí¬ ê³ ë ¤)
    batch_size = 20  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
    total_uploaded = 0
    
    print(f"   ğŸš€ ì—…ë¡œë“œ ì‹œì‘ (ì´ {len(documents)}ê°œ ì²­í¬)")
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€ìš© - ì„ íƒ ì‚¬í•­)
    supabase.table("documents").delete().eq("ticker", ticker).execute()
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings_response = openai_client.embeddings.create(
                input=[doc["content"] for doc in batch],
                model="text-embedding-3-small"
            )
            
            # ë ˆì½”ë“œì— ì„ë² ë”© ì¶”ê°€
            records = []
            for j, doc in enumerate(batch):
                doc["embedding"] = embeddings_response.data[j].embedding
                records.append(doc)
            
            # Supabase ì €ì¥
            supabase.table("documents").insert(records).execute()
            
            total_uploaded += len(batch)
            print(f"      Running... ({total_uploaded}/{len(documents)})", end="\r")
            
            # Rate limit ë°©ì§€
            time.sleep(0.5)
            
        except Exception as e:
            print(f"\n   âŒ ì˜¤ë¥˜ ë°œìƒ (Batch {i}): {e}")
            time.sleep(5)
            
    print(f"\n   âœ… {ticker} ì™„ë£Œ: {total_uploaded}ê°œ ì²­í¬ ì €ì¥ë¨")

def main():
    print("="*60)
    print("ğŸ§  10-K ë¬¸ì„œ ì„ë² ë”© ë° Supabase ì—…ë¡œë“œ")
    print("="*60)
    
    if not DATA_DIR.exists():
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        return

    # ì²˜ë¦¬ëœ ê¸°ì—… ëª©ë¡ ë¡œë“œ
    processed_companies_path = DATA_DIR / "processed_companies.csv"
    if processed_companies_path.exists():
        companies_df = pd.read_csv(processed_companies_path)
        tickers = companies_df["ticker"].tolist()
    else:
        # ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ í™•ì¸
        tickers = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]
    
    print(f"ğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ: {len(tickers)}ê°œ ê¸°ì—…")
    
    for ticker in tickers:
        company_dir = DATA_DIR / ticker
        if company_dir.exists():
            try:
                process_company_documents(ticker, company_dir)
            except Exception as e:
                print(f"âŒ {ticker} ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
